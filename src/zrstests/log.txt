ZRS Replication Log
===================

A ZRS replication log is a colection of files that record replication
data records.  If a database is lost or damaged, it can be rebuilt by
applying records from the replication log.  A replication log can also be
used for auditing purposes, as it can record every transaction written
to a database.

Two programs are provided creating and for replaying a log.  The
recorder is run with a ZRS replication address, a file size,
and a destination directory.  The recorder script simply creates a
recorder object. Let's create a recoder object manually:

    >>> import os, zc.zrs.log
    >>> os.mkdir('log')
    >>> recorder = zc.zrs.log.Recorder(
    ...     ('', 8000), 'log', 1000, reactor=reactor,
    ...     keep_alive_delay=60, check_checksums=True)
    INFO zc.zrs.log:
    Opening ZRSReplicationLog('log') ('', 8000)
    INFO zc.zrs.reactor:
    Starting factory <zc.zrs.log.RecorderFactory instance>

We supplied a replication address, a destination directiory for the
log, and a log file size.  Log files are closed and new files reopened
when the the file size is exceeded after writing a transaction.  For
testing purposes, we also supplied a test reactor, so we can control
and see the network activity.

We also specified a keep-alive delay, which we'll say more about
below, and a flag instructing the recorder to check checksums from the
primary.  Checking of checksums is optional because older versions of
ZRS to supply checksums.

Let's look at the log directory:

    >>> def listdir(dname):
    ...     for name in sorted(os.listdir(dname)):
    ...         print name, os.stat(os.path.join(dname, name)).st_size

    >>> listdir('log')
    0000000000000000 0

Here, we have a file who's name consists of 16 0's and with a size of
0.  The file name is the starting time stamp, as a hex string.  We
don't have any data yet. Our demo reactor doesn't make any network
connections, so of course, the log doesn't have any data yet.  The
demo reactor simulates connections by creatiing client connections
when we call accept.  Let's accept the connection, getting the client
connection created for this server:

    >>> connection = reactor.accept()
    INFO zc.zrs.log:
    IPv4Address(TCP, '127.0.0.1', 47245): Connected

The recorder writes two messages to the connection to begin
the interaction with the server.  The first is the ZRS protocol
identifier:

    >>> connection.read()
    'zrs2.1'

The second message is the identifier of the last transaction written
to the log.  This log doesn't have any data, so we'll get a
transaction id consisting of all zeros:

    >>> connection.read()
    '\x00\x00\x00\x00\x00\x00\x00\x00'

This is all the data sent by the recoder.  From this point on, the
recorder passively waits for data sent by the server.  We'll pass it
some data.  To do this, we'll get some help from a file-storage
iterator that will give us the data we need to send.  We'll create a
file storage and database that we can use for this:

    >>> import ZODB.FileStorage
    >>> primary_fs = ZODB.FileStorage.FileStorage(
    ...     'primary.fs', blob_dir='primary_blobs')
    >>> import zc.zrs.primary
    >>> primary_data = zc.zrs.primary.FileStorageIterator(primary_fs)

    >>> from ZODB.DB import DB
    >>> primary_db = DB(primary_fs)

The first transaction initializes the database.  Let's send that to
the secondary.

    >>> trans = primary_data.next()

Transactions are sent as a series of messages.  To make this easier
for this demonstration, the test connections let us pass data directly.

The first message is a transaction message that starts the transaction
and includes transaction meta data.

    >>> connection.send(('T', (trans.tid, trans.status, trans.user,
    ...                        trans.description, trans._extension)))

This initial message is followed by a serious of messages, two for
each object store. The first has record meta data as a pickle, and the
second has the record data:

    >>> for record in trans:
    ...     connection.send(('S', (record.oid, record.tid, record.version,
    ...                            record.data_txn)))
    ...     connection.send(record.data, raw=True)

Finally, a commit message is sent marking the end of the transaction:

    >>> connection.send(('C', (connection.md5.digest(),)))

If we look at the log file, we can see we have some data now.

    >>> listdir('log')
    0000000000000000 188

Let's send some more data.

    >>> primary_conn = primary_db.open()
    >>> proot = primary_conn.root()
    >>> import persistent.mapping
    >>> proot['x'] = persistent.mapping.PersistentMapping()
    >>> commit()

    >>> blob_block_size = 1 << 16
    >>> def send_transaction():
    ...     global trans
    ...     trans = primary_data.next()
    ...     connection.send(('T', (trans.tid, trans.status, trans.user,
    ...                            trans.description, trans._extension)))
    ...     for record in trans:
    ...         if zc.zrs.primary.is_blob_record(record.data):
    ...             try:
    ...                 fname = primary_fs.loadBlob(
    ...                     record.oid, record.tid)
    ...                 f = open(fname, 'rb')
    ...             except (IOError, ZODB.POSException.POSKeyError):
    ...                 pass
    ...             else:
    ...                 f.seek(0, 2)
    ...                 blob_size = f.tell()
    ...                 blocks, r = divmod(blob_size, blob_block_size)
    ...                 if r:
    ...                     blocks += 1
    ...                 connection.send(('B',
    ...                                 (record.oid, record.tid, record.version,
    ...                                  record.data_txn, long(blocks))))
    ...                 connection.send(record.data, raw=True)
    ...                 f.seek(0)
    ...                 while blocks > 0:
    ...                     data = f.read(blob_block_size)
    ...                     if not data:
    ...                         raise AssertionError("Too much blob data")
    ...                     blocks -= 1
    ...                     connection.send(data, raw=True)
    ...                 f.close()
    ...                 continue
    ...         connection.send(('S', (record.oid, record.tid,
    ...                                record.version,
    ...                               record.data_txn)))
    ...         connection.send(record.data, raw=True)
    ...     connection.send(('C', (connection.md5.digest(),)))

    >>> send_transaction()

    >>> listdir('log')
    0000000000000000 478

Now, let's create some blob data:

    >>> proot['blob'] = ZODB.blob.Blob()
    >>> proot['blob'].open('w').write('test\n')
    >>> commit()
    >>> send_transaction()

    >>> listdir('log')
    0000000000000000 786

Let's create some more interesting blob data.

    >>> import random, struct
    >>> random.seed(0)
    >>> for i in range(5):
    ...     wdata = ''.join(struct.pack(">I", random.randint(0, 1<<32))
    ...                     for i in range(random.randint(1000, 100000)))
    ...     proot[i] = ZODB.blob.Blob()
    ...     proot[i].open('w').write(wdata)
    ...     commit()
    ...     send_transaction()

    >>> proot['empty'] = ZODB.blob.Blob()
    >>> commit()
    >>> send_transaction()

    >>> proot['empty'].open('w').write('new data')
    >>> commit()
    >>> send_transaction()

    >>> listdir('log')
    0000000000000000 339545
    036c6b9100000000 37629
    036c6b9104444444 147042
    036c6b9108888888 168337
    036c6b910ccccccc 123639
    036c6b9111111111 554

Here we see we generated enough data to get multiple files.  Note that
the files are much larger than the maximum size of 1000 we passed to
the Record constructor.  This is because transactions aren't split
accross files.


Replay
--------

We can replay the log using the replay function. The function takes a
log directory and a storage to replay to:

    >>> fs = ZODB.FileStorage.FileStorage('Data.fs', blob_dir='blobs')
    >>> zc.zrs.log.replay('log', fs)
    >>> fs.close()

    >>> open('Data.fs', 'rb').read() == open('primary.fs', 'rb').read()
    True
    >>> for dpath,  dirnames, filenames in os.walk('primary_blobs'):
    ...     rpath = dpath.replace('primary_', '')
    ...     for name in dirnames:
    ...         path = os.path.join(rpath, name)
    ...         if not os.path.exists(path):
    ...             raise ValueError(path)
    ...     for name in filenames:
    ...         ppath = os.path.join(dpath, name)
    ...         path = os.path.join(rpath, name)
    ...         if open(path, 'rb').read() != open(ppath, 'rb').read():
    ...             print ValueError("different", path, ppath)


Keep alive
----------

Recorders send keep-alive messages to primaries.  This
can help in flaky network configurations in which connections are
dropped in odd ways after periods of inactivity.

    >>> reactor.later # doctest: +NORMALIZE_WHITESPACE
    [<1 60 keep_alive () {}>]

Here we see the first entry is the keep alive. Let's tell the reactor
to flush it's qeueue

    >>> reactor.doLater()

Now an empty strings has been semd to the primary:

    >>> connection.read()
    ''

And a new keep alive has been scheduled:

    >>> reactor.later # doctest: +NORMALIZE_WHITESPACE
    [<2 60 keep_alive () {}>]

Connection Failures
-------------------

If a connection fails, then the recorder will disconnect and and ask
the reactor to reconnect 60 seconds later.  We'll demonstrate this by
asking our test conection to fail:

    >>> connection.fail()
    INFO zc.zrs.log:
    IPv4Address(TCP, '127.0.0.1', 47245): Disconnected 'failed'
    INFO zc.zrs.reactor:
    Stopping factory <zc.zrs.log.RecorderFactory instance>

Our test reactor just queues later attempts:

    >>> reactor.later # doctest: +NORMALIZE_WHITESPACE
    [<3 60 connect () {}>]

Note that our keep alive job has disappeared. This is because the keep
alive is removed when a recorder is disconnected.

We'll simulate calling these later:

    >>> reactor.doLater()
    INFO zc.zrs.reactor:
    Starting factory <zc.zrs.log.RecorderFactory instance>

    >>> reactor.later # doctest: +NORMALIZE_WHITESPACE
    []

This causes a connection to be attempted, which we'll accept:

    >>> connection = reactor.accept()
    INFO zc.zrs.log:
    IPv4Address(TCP, '127.0.0.1', 47246): Connected

The secondary will send the protocol and a transaction, as before:

    >>> connection.read()
    'zrs2.1'

    >>> tid = connection.read()
    >>> tid
    '\x03lk\x91\x19\x99\x99\x99'

    >>> connection.init_md5(tid)

Note that the transaction id isn't zero.  It fact, it's the same as
the last transaction that was sent to the secondary:

    >>> tid == trans.tid
    True

Let's create a new transaction.

    >>> proot['y'] = persistent.mapping.PersistentMapping()
    >>> commit()

We'll send some of the data but not the final commit record to the recorder
and then break the connection:

    >>> newtrans = primary_data.next()
    >>> connection.send(('T', (newtrans.tid, newtrans.status, newtrans.user,
    ...                        newtrans.description, newtrans._extension)))
    >>> record = newtrans.next()
    >>> connection.send(('S', (record.oid, record.tid, record.version,
    ...                        record.data_txn)))
    >>> connection.send(record.data, raw=True)

If we look at the log directory, we'll see the last log file has grown:

    >>> recorder._storage.file.flush()
    >>> listdir('log')
    0000000000000000 339545
    036c6b9100000000 37629
    036c6b9104444444 147042
    036c6b9108888888 168337
    036c6b910ccccccc 123639
    036c6b9111111111 901

But if the connection is lost, the trailing incomplete transaction
will be lost:

    >>> connection.fail()
    INFO zc.zrs.log:
    IPv4Address(TCP, '127.0.0.1', 47246): Disconnected 'failed'
    INFO zc.zrs.reactor:
    Stopping factory <zc.zrs.log.RecorderFactory instance>

The log still shows the same data we had before:

    >>> listdir('log')
    0000000000000000 339545
    036c6b9100000000 37629
    036c6b9104444444 147042
    036c6b9108888888 168337
    036c6b910ccccccc 123639
    036c6b9111111111 554

Again, the secondary attempts to reconnect.  We'll accept the new connection.

    >>> reactor.later # doctest: +NORMALIZE_WHITESPACE
    [<5 60 connect () {}>]

The reason the record increased from 3 to 5 is that a keep-alive
record was added, but then removed when we disconnected.

    >>> reactor.doLater()
    INFO zc.zrs.reactor:
    Starting factory <zc.zrs.log.RecorderFactory instance>

    >>> reactor.later
    []

    >>> connection = reactor.accept()
    INFO zc.zrs.log:
    IPv4Address(TCP, '127.0.0.1', 47247): Connected

    >>> connection.read()
    'zrs2.1'

    >>> connection.read() == trans.tid
    True

    >>> connection.init_md5(trans.tid)


Again, the transaction id sent is the earlier transaction, which is
the last transaction committed by the secondary.

We'll send the new data and this time send the commit record:

    >>> connection.send(('T', (newtrans.tid, newtrans.status, newtrans.user,
    ...                        newtrans.description, newtrans._extension)))
    >>> connection.send(('S', (record.oid, record.tid, record.version,
    ...                        record.data_txn)))
    >>> connection.send(record.data, raw=True)

    >>> record = newtrans.next()

    >>> connection.send(('S', (record.oid, record.tid, record.version,
    ...                        record.data_txn)))
    >>> connection.send(record.data, raw=True)

    >>> connection.send(('C', (connection.md5.digest(),)))

Now, if we replay again, we'll see that our replay log and the
original are back in sync:

    >>> fs = ZODB.FileStorage.FileStorage('Data.fs', blob_dir='blobs',
    ...                                   create=True)
    >>> zc.zrs.log.replay('log', fs)
    >>> fs.close()
    >>> open('Data.fs', 'rb').read() == open('primary.fs', 'rb').read()
    True

So far we've seen the effect of breaking successful connections.
Secondaries also try to reconnect if connections are rejected.  We'll
again break the connection, but this time we'll reject the connection
attempt:

    >>> connection.fail()
    INFO zc.zrs.log:
    IPv4Address(TCP, '127.0.0.1', 47247): Disconnected 'failed'
    INFO zc.zrs.reactor:
    Stopping factory <zc.zrs.log.RecorderFactory instance>

    >>> reactor.later # doctest: +NORMALIZE_WHITESPACE
    [<7 60 connect () {}>]

    >>> reactor.doLater()
    INFO zc.zrs.reactor:
    Starting factory <zc.zrs.log.RecorderFactory instance>

    >>> reactor.later
    []

    >>> connection = reactor.reject()
    INFO zc.zrs.reactor:
    Stopping factory <zc.zrs.log.RecorderFactory instance>

As with failed connections, the recorder just tries again, waiting a
minute between attempts.

    >>> reactor.later # doctest: +NORMALIZE_WHITESPACE
    [<8 60 connect () {}>]

    >>> reactor.doLater()
    INFO zc.zrs.reactor:
    Starting factory <zc.zrs.log.RecorderFactory instance at 0xb6c31f2c>

Let's look at what would happen if the recorder died uncleanly. We'll
have to do some strange gymnastics to simulate this.  First, we'll
connect and send a partial transaction:

    >>> connection = reactor.accept()
    INFO zc.zrs.log:
    IPv4Address(TCP, '127.0.0.1', 47249): Connected

    >>> connection.read()
    'zrs2.1'

    >>> tid = connection.read()
    >>> tid
    '\x03lk\x91\x1d\xdd\xdd\xdd'

    >>> connection.init_md5(tid)

    >>> tid == newtrans.tid
    True

    >>> listdir('log')
    0000000000000000 339545
    036c6b9100000000 37629
    036c6b9104444444 147042
    036c6b9108888888 168337
    036c6b910ccccccc 123639
    036c6b9111111111 1027
    036c6b911ddddddd 0

    >>> proot['y'][0] = ''
    >>> commit()
    >>> newtrans = primary_data.next()
    >>> connection.send(('T', (newtrans.tid, newtrans.status, newtrans.user,
    ...                        newtrans.description, newtrans._extension)))
    >>> record = newtrans.next()
    >>> connection.send(('S', (record.oid, record.tid, record.version,
    ...                        record.data_txn)))
    >>> connection.send(record.data, raw=True)

We see partial data in the last log file:

    >>> recorder._storage.file.flush()
    >>> listdir('log')
    0000000000000000 339545
    036c6b9100000000 37629
    036c6b9104444444 147042
    036c6b9108888888 168337
    036c6b910ccccccc 123639
    036c6b9111111111 1027
    036c6b911ddddddd 148

Let's save that data:

    >>> data = open(os.path.join('log', '036c6b911ddddddd')).read()

If we close the recorder, the file will be returned to it's previous
size:

    >>> recorder.close() # doctest: +NORMALIZE_WHITESPACE
    INFO zc.zrs.log:
    Closing ZRSReplicationLog('log') ('', 8000)
    INFO zc.zrs.log:
    IPv4Address(TCP, '127.0.0.1', 47249):
    Disconnected <twisted.python.failure.Failure
    twisted.internet.error.ConnectionDone>

    >>> listdir('log')
    0000000000000000 339545
    036c6b9100000000 37629
    036c6b9104444444 147042
    036c6b9108888888 168337
    036c6b910ccccccc 123639
    036c6b9111111111 1027
    036c6b911ddddddd 0

Now, we'll restore it:

    >>> open(os.path.join('log', '036c6b911ddddddd'), 'wb').write(data)

    >>> listdir('log')
    0000000000000000 339545
    036c6b9100000000 37629
    036c6b9104444444 147042
    036c6b9108888888 168337
    036c6b910ccccccc 123639
    036c6b9111111111 1027
    036c6b911ddddddd 148

Now, if we recreate the recorder, it will restore the file to the size
it was before:

    >>> recorder = zc.zrs.log.Recorder(
    ...     ('', 8000), 'log', 1000, reactor=reactor,
    ...     keep_alive_delay=60, check_checksums=True)
    INFO zc.zrs.log:
    Opening ZRSReplicationLog('log') ('', 8000)
    INFO zc.zrs.reactor:
    Starting factory <zc.zrs.log.RecorderFactory instance at 0xb6461bec>

    >>> listdir('log')
    0000000000000000 339545
    036c6b9100000000 37629
    036c6b9104444444 147042
    036c6b9108888888 168337
    036c6b910ccccccc 123639
    036c6b9111111111 1027
    036c6b911ddddddd 0

Similarly, if the file is broken, it will be removed and recreated:

    >>> recorder.close() # doctest: +NORMALIZE_WHITESPACE
    INFO zc.zrs.log:
    Closing ZRSReplicationLog('log') ('', 8000)
    INFO zc.zrs.reactor:
    Stopping factory <zc.zrs.log.RecorderFactory instance at 0xb6475bec>


    >>> open(os.path.join('log', '036c6b911ddddddd'), 'wb').write(data[:-2])

    >>> listdir('log')
    0000000000000000 339545
    036c6b9100000000 37629
    036c6b9104444444 147042
    036c6b9108888888 168337
    036c6b910ccccccc 123639
    036c6b9111111111 1027
    036c6b911ddddddd 146

    >>> recorder = zc.zrs.log.Recorder(
    ...     ('', 8000), 'log', 1000, reactor=reactor,
    ...     keep_alive_delay=60, check_checksums=True)
    ... # doctest: +ELLIPSIS
    ERROR zc.zrs.log:
    Error reading log file log/036c6b911ddddddd
    Traceback (most recent call last):
    ...
    IOError: [Errno 22] Invalid argument
    INFO zc.zrs.log:
    Opening ZRSReplicationLog('log') ('', 8000)
    INFO zc.zrs.reactor:
    Starting factory <zc.zrs.log.RecorderFactory instance at 0xb63fbf0c>

    >>> listdir('log')
    0000000000000000 339545
    036c6b9100000000 37629
    036c6b9104444444 147042
    036c6b9108888888 168337
    036c6b910ccccccc 123639
    036c6b9111111111 1027
    036c6b911ddddddd 0

Finally, lets look at a situation in which the last file has more than
one transaction:

    >>> connection = reactor.accept()
    INFO zc.zrs.log:
    IPv4Address(TCP, '127.0.0.1', 47251): Connected

    >>> connection.read()
    'zrs2.1'

    >>> tid = connection.read()

    >>> connection.init_md5(tid)

    >>> primary_data = zc.zrs.primary.FileStorageIterator(primary_fs, start=tid)

    >>> send_transaction()

    >>> proot['y'][0] = ''
    >>> commit()

    >>> newtrans = primary_data.next()
    >>> connection.send(('T', (newtrans.tid, newtrans.status, newtrans.user,
    ...                        newtrans.description, newtrans._extension)))
    >>> record = newtrans.next()
    >>> connection.send(('S', (record.oid, record.tid, record.version,
    ...                        record.data_txn)))
    >>> connection.send(record.data, raw=True)

    >>> recorder._storage.file.flush()
    >>> listdir('log')
    0000000000000000 339545
    036c6b9100000000 37629
    036c6b9104444444 147042
    036c6b9108888888 168337
    036c6b910ccccccc 123639
    036c6b9111111111 1027
    036c6b911ddddddd 313

    >>> data = open(os.path.join('log', '036c6b911ddddddd'), 'rb').read()

    >>> recorder.close() # doctest: +NORMALIZE_WHITESPACE
    INFO zc.zrs.log:
    Closing ZRSReplicationLog('log') ('', 8000)
    INFO zc.zrs.log:
    IPv4Address(TCP, '127.0.0.1', 47251):
    Disconnected <twisted.python.failure.Failure
    twisted.internet.error.ConnectionDone>

    >>> listdir('log')
    0000000000000000 339545
    036c6b9100000000 37629
    036c6b9104444444 147042
    036c6b9108888888 168337
    036c6b910ccccccc 123639
    036c6b9111111111 1027
    036c6b911ddddddd 165

    >>> open(os.path.join('log', '036c6b911ddddddd'), 'wb').write(data)
    >>> listdir('log')
    0000000000000000 339545
    036c6b9100000000 37629
    036c6b9104444444 147042
    036c6b9108888888 168337
    036c6b910ccccccc 123639
    036c6b9111111111 1027
    036c6b911ddddddd 313

    >>> recorder = zc.zrs.log.Recorder(
    ...     ('', 8000), 'log', 1000, reactor=reactor,
    ...     keep_alive_delay=60, check_checksums=True)
    INFO zc.zrs.log:
    Opening ZRSReplicationLog('log') ('', 8000)
    INFO zc.zrs.reactor:
    Starting factory <zc.zrs.log.RecorderFactory instance at 0xb642798c>

    >>> listdir('log')
    0000000000000000 339545
    036c6b9100000000 37629
    036c6b9104444444 147042
    036c6b9108888888 168337
    036c6b910ccccccc 123639
    036c6b9111111111 1027
    036c6b911ddddddd 165

Let's commit the last transaction and make sure replay can recreate
the original database:


    >>> connection = reactor.accept()
    INFO zc.zrs.log:
    IPv4Address(TCP, '127.0.0.1', 47252): Connected

    >>> connection.read()
    'zrs2.1'

    >>> tid = connection.read()

    >>> connection.init_md5(tid)

    >>> primary_data = zc.zrs.primary.FileStorageIterator(primary_fs, start=tid)

    >>> send_transaction()

    >>> fs = ZODB.FileStorage.FileStorage('Data.fs', blob_dir='blobs',
    ...                                   create=True)
    >>> zc.zrs.log.replay('log', fs)
    >>> fs.close()

    >>> open('Data.fs', 'rb').read() == open('primary.fs', 'rb').read()
    True

.. cleanup

    >>> primary_db.close()

    >>> recorder.close() # doctest: +NORMALIZE_WHITESPACE
    INFO zc.zrs.log:
    Closing ZRSReplicationLog('log') ('', 8000)
    INFO zc.zrs.log:
    IPv4Address(TCP, '127.0.0.1', 47252):
    Disconnected <twisted.python.failure.Failure
    twisted.internet.error.ConnectionDone>

    >>> reactor.doLater()

    >>> reactor.later
    []

    Because we closed the secondary, the connector doesn't try to connect:

    >>> reactor.clients
    []
