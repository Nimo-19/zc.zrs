File Storage Iterator
=====================

The file-storage iterator defined here is similar to the normal
iterator provided with FileStorage, with some important exceptions:

1. The iterator described here remains connected to the file storage.
   It blocks waiting for input and generates data as input are written
   to the file storage.  It blocks using a condition passed to it.
   The condition is notified when there is new data.  Multiple
   iterators will often wait on the same condition.

2. The iterator is a bit smarter about initialization with a
   transaction. In particular, it will search from the end of the file
   if the requested transaction is closer to the last transaction than
   to the first transaction.

To see this in action, We'll create a FileStorage and add some data to
it. 

    >>> import ZODB.FileStorage
    >>> from ZODB.DB import DB
    >>> import persistent.dict

    >>> fs = ZODB.FileStorage.FileStorage('Data.fs')
    >>> db = DB(fs)
    >>> conn = db.open()

    >>> for i in range(10):
    ...     conn.root()[i] = persistent.dict.PersistentDict()
    ...     commit()

We'll create a simple program that iterates over a file storage
printing some information.

    >>> import zc.zrs.fsiterator
    >>> from ZODB.TimeStamp import TimeStamp
    >>> from ZODB import utils
    >>> def list(*iterator_args):
    ...     for trans in zc.zrs.fsiterator.FileStorageIterator(*iterator_args):
    ...         print TimeStamp(trans.tid), [utils.u64(r.oid) for r in trans]

 Because this is going to block when it gets to the end of the file,
 we'll run it in a thread.  We're also going to use a special test
 condition that takes an event that we can use to wait for the thread
 to do work.

    >>> import threading

    >>> class Condition:
    ... 
    ...     def __init__(self, event):
    ...         self.event = event
    ...         self.condition = threading.Condition()
    ...         self.acquire = self.condition.acquire
    ...         self.release = self.condition.release
    ...         self.notifyAll = self.condition.notifyAll
    ...         self.should_stop = False
    ... 
    ...     def wait(self):
    ...         if self.should_stop:
    ...             raise StopIteration
    ...         self.event.set()
    ...         self.condition.wait()
    ... 
    ...     def stop(self):
    ...         self.should_stop = True
    ...         self.acquire()
    ...         self.condition.notifyAll()
    ...         self.release()
    ...
    ...     def notify_and_wait(self):
    ...         self.event.clear()
    ...         self.acquire()
    ...         self.notifyAll()
    ...         self.release()
    ...         self.event.wait()

    >>> event = threading.Event()
    >>> condition = Condition(event)
    >>> thread = threading.Thread(target=list, args = (fs, condition))
    >>> thread.setDaemon(True)

    >>> thread.start(); event.wait()
    2007-03-21 20:32:57.000000 [0L]
    2007-03-21 20:32:58.000000 [0L, 1L]
    2007-03-21 20:32:59.000000 [0L, 2L]
    2007-03-21 20:33:00.000000 [0L, 3L]
    2007-03-21 20:33:01.000000 [0L, 4L]
    2007-03-21 20:33:02.000000 [0L, 5L]
    2007-03-21 20:33:03.000000 [0L, 6L]
    2007-03-21 20:33:04.000000 [0L, 7L]
    2007-03-21 20:33:05.000000 [0L, 8L]
    2007-03-21 20:33:06.000000 [0L, 9L]
    2007-03-21 20:33:07.000000 [0L, 10L]


We can write addition al data to the file storage:

    >>> conn.root()[10] = persistent.dict.PersistentDict()
    >>> commit()

We won't see any changes in the iterator until we call notifyAll on
the condition. (We'll use the helper function notify_and_wait, which
calls notifyAll correctly and then waits on out event.

    >>> condition.notify_and_wait()
    2007-03-21 20:33:08.000000 [0L, 11L]

If we pack the file storage:

    >>> import time, ZODB.serialize
    >>> fs.pack(time.time()-5, ZODB.serialize.referencesf)

and write more data, we'll still see the data, even though the file
storage is now using a new file:

    >>> conn.root()[11] = persistent.dict.PersistentDict()
    >>> commit()
    >>> condition.notify_and_wait()
    2007-03-21 20:33:09.000000 [0L, 12L]

When we create iterators, we can pass a starting transaction id. This
must be less than or equal to any transaction in the database, or
ZODB.utils.z64 (all nulls) if the database is empty.  Passing an ltid
that is too high, will cause an error:

    >>> def tid_from_time(t):
    ...     return repr(TimeStamp(*(time.gmtime(t)[:5] + (t%60,))))

    >>> tid = tid_from_time(time.time()+1)
    >>> zc.zrs.fsiterator.FileStorageIterator(fs, condition, tid)
    Traceback (most recent call last):
    ...
    TidTooHigh: '\x03lk\x91*\xaa\xaa\xaa'

If the tis is lower than any tid in the file, then iteration will
start at the beginning:

    >>> tid = tid_from_time(time.time()-100)
    >>> it = zc.zrs.fsiterator.FileStorageIterator(fs, condition, tid)
    >>> trans = it.next()
    >>> print TimeStamp(trans.tid), [utils.u64(r.oid) for r in trans]
    2007-03-21 20:32:58.000000 [1L]

(You might be wondering why we didn't get the first transaction
written. The answer is because it, and part of the first transaction
were packed away.

    >>> trans.status
    'p'

:)

If the tid lies somewhere in the middle, then iteration will start
there:

    >>> tid = tid_from_time(time.time()-3)
    >>> it = zc.zrs.fsiterator.FileStorageIterator(fs, condition, tid)
    >>> trans = it.next()
    >>> print TimeStamp(trans.tid), [utils.u64(r.oid) for r in trans]
    2007-03-21 20:33:07.000000 [0L, 10L]


.. Clean up

    >>> condition.stop()
    >>> thread.join()
